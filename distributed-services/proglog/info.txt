
+------+
| gRPC |
+------+
1- each RPC line is an endpoint in that service, The requests 
   and responses are messages that the compiler turns into Go structs,   
2- ConsumeStream—a server-side streaming RPC where the client sends a
   request to the server and gets back a stream to read a sequence of messages
3- ProduceStream—a bidirectional streaming RPC where both the client and
    server send a sequence of messages using a read-write stream. The two
    streams operate independently, so the clients and servers can read and
    write in whatever order they like. For example, the server could wait to
    receive all of the client’s requests before sending back its response. You’d
    order your calls this way if your server needed to process the requests in
    batches or aggregate a response over multiple requests. Alternatively, the
    server could send back a response for each request in lockstep. You’d
    order your calls this way if each request required its own corresponding response.

+-----+
| TLS |
+-----+
CFSSL has two tools we’ll need:
    • cfssl to sign, verify, and bundle TLS certificates and output the results as JSON. 
    • cfssljson to take that JSON output and split them into separate key, cer- tificate, CSR, and bundle files.
in ca-csr.json 
 ╒══════════════════════════════════════════════════════════════════════════════════╕
   • C—country                                                                      
   • L—locality or municipality (such as city)                                      
   • ST—state or province                                                           
   • O—organization                                                                 
   • OU—organizational unit (such as the department responsible for owning the key) 
 ╘══════════════════════════════════════════════════════════════════════════════════╛

The simplest way to implement authorization is with an access control list (ACL). An ACL is a table 
  of rules where each row says something like “Subject A is permitted to do action B on object 
  C.” For example: Alice is permitted to read Distributed Services with Go. In this example, 
  Alice is the subject, to read is the action, and Distributed Services with Go is the object.

+-----------------------------------------------------------------------------+
| The model.conf configures Casbin to use ACL as its authorization mechanism. |
+-----------------------------------------------------------------------------+

╒════════════════════════════════════════════════════════════════════════════════════════════════════════╕
  In the policy.csv file, this is an ACL table, with two entries saying that the root client has produce 
  and consume permissions on the * object (which we’re using as a wildcard,                              
  meaning any object). All other clients, including nobody, will be denied.                              
╘════════════════════════════════════════════════════════════════════════════════════════════════════════╛


╒═════════╕
│ Metrics │
╘═════════╛
Metrics measure numeric data over time, such as how many requests failed or how long each request took.
  Metrics like these help to define service-level indicators (SLI), objectives (SLO), and agreements (SLA). 
  You’ll use metrics to report the health of your system, trigger internal alerts, and graph on
  dashboards to get an idea of how your system’s doing at a glance. there are three types of metrics:

Counters
    Counters track the number of times an event happened, such as the
    number of requests that failed or the sum of some fact of your system like the number of bytes processed.
    You’ll often take a counter and use it to get a rate: the number of times an event happened in an interval
    Who cares about the total requests we’ve received other than to brag about it? What we care about is how
    many requests we’ve handled in the past second or minute—if that dropped significantly you’d want to check 
    for latency in your system. You’d want to know when your request error rate spikes so you can see
    what’s wrong and fix it.
Histograms
    Histograms show you a distribution of your data. You’ll mainly use histograms 
    for measuring the percentiles of your request duration and sizes.
Gauges
    Gauges track the current value of something. You can replace that value entirely.
    Gauges are useful for saturation-type metrics, like a host’s disk usage percentage 
    or the number of load balancers compared to your cloud provider’s limits.

┌───────────────────────────────────────────┐
  Google’s four golden signals1 to measure: 
└───────────────────────────────────────────┘
• Latency: the time it takes your service to process requests. If your latency spikes, you often need 
    to scale your system vertically by changing to an instance with more memory, CPUs, or IOPS, 
    or scale your system horizontally by adding more instances to your load balancer

• Traffic: the amount of demand on your service. For a typical web service,
    this could be requests processed per second. For an online video game or video streaming service, 
    it could be the number of concurrent users. These metrics are good for bragging rights (hopefully), 
    but more important, they can help give you an idea of the scale at which youu’re 
    working and when you’ve scaled to the point you need a new design.
    
• Errors: your service’s request failure rate. Internal server errors are par-ticularly important.

• Saturation—a measure of your service’s capacity. For example, if your
    service persists data to disk, at your current ingress rate will you run out of hard drive space soon?
    If you have an in-memory store, how much memory is your service using compared to the memory available?

• Traces comprise one or more spans. Spans can have parent/child relationships
    or be linked as siblings. Each span represents a part of the request’s execution.

OpenTelemetry8 is a Cloud Native Computing Foundation (CNCF) project that provides robust and 
    portable APIs and libraries that we can use for metrics and distributed tracing in our service. 
    (OpenCensus and OpenTracing merged to form OpenTelemetry, which is backward-compatible with existing 
    Open-Census integrations.)  OpenTelemetry’s Go gRPC integration supports traces but not metrics, 



When a test file implements TestMain(m *testing.M), Go will call TestMain(m) instead
    of running the tests directly. TestMain() gives us a place for setup that applies to all tests in that 
    file, like enabling our debug output. Flag parsing has to go in TestMain() instead of init(), 
    otherwise Go can’t define the flag and your code will error and exit


When you have an application that needs to talk to a service, the tool you
use for service discovery needs to perform the following tasks:
    • Manage a registry of services containing info such as their IPs and ports;
    • Help services find other services using the registry;
    • Health check service instances and remove them if they’re not well; and
    • Deregister services when they go offline.

Serf maintains cluster membership by using an efficient, lightweight gossip
    protocol to communicate between the service’s nodes. Unlike service registry
    projects like ZooKeeper and Consul, Serf doesn’t have a central-registry architectural style.
    Instead, each instance of your service in the cluster runs as a Serf node.
To implement service discovery with Serf we need to:
    1. Create a Serf node on each server.
    2. Configure each Serf node with an address to listen on and accept connec-tions from other Serf nodes.
    3. Configure each Serf node with addresses of other Serf nodes and join their cluster.
    4. Handle Serf’s cluster discovery events, such as when a node joins or fails in the cluster.

Serf has a lot of configurable parameters, but the five parameters you’ll typically use are:
    • NodeName—the node name acts as the node’s unique identifier across the
      Serf cluster. If you don’t set the node name, Serf uses the hostname.
      
    • BindAddr and BindPort—Serf listens on this address and port for gossiping.
    
    • Tags—Serf shares these tags to the other nodes in the cluster and should
      use these tags for simple data that informs the cluster how to handle this
      node. For example, Consul shares each node’s RPC address with Serf
      tags, and once they know each other’s RPC address, they can make RPCs
      to each other. Consul shares whether the node is a voter or non-voter, which changes the node’s
      role in the Raft cluster. In our code, similar to Consul, we’ll share each node’s user 
      configured RPC address with a Serf tag so the nodes know which addresses to send their RPCs.
      
    • EventCh—the event channel is how you’ll receive Serf’s events when a node joins or leaves the cluster.
      If you want a snapshot of the members at any point in time, you can call Serf’s Members() method.
      
    • StartJoinAddrs—when you have an existing cluster and you create a new node
      that you want to add to that cluster, you need to point your new node to
      at least one of the nodes now in the cluster. After the new node connects
      to one of those nodes in the existing cluster, it’ll learn about the rest of
      the nodes, and vice versa (the existing nodes learn about the new node).
      The StartJoinAddrs field is how you configure new nodes to join an existing
      cluster. You set the field to the addresses of nodes in the cluster, and
      Serf’s gossip protocol takes care of the rest to join your node to the cluster.
      In a production environment, specify at least three addresses to make
      your cluster resilient to one or two node failures or a disrupted network.
      


 +-----------------------------------------------------------------+
 | Consensus algorithms are tools used to get distributed services |
 | to agree on shared state even in the face of failures           |
 +-----------------------------------------------------------------+

What Is Raft and How Does It Work?
  Raft is a distributed consensus algorithm designed to be easily understood and implemented. It’s the 
  consensus algorithm behind services like Etcd—the distributed key-value store that backs Kubernetes,
  Consul, and soon Kafka whose team is migrating from ZooKeeper to Raft.
  
- A Raft cluster has one leader and the rest of the servers are followers. The leader maintains power by 
    sending heartbeat requests to its followers If the follower times out waiting for a heartbeat request 
    from the leader, then the follower becomes a candidate and begins an election to decide the next leader
    The candidate votes for itself and then requests votes from the followers
    If the candidate receives a majority of the votes, it becomes the leader, and it sends 
    heartbeat requests to the followers to establish authorit

    Followers can become candidates simultaneously if they time out at the same time waiting for the 
    leader’s heartbeats. They’ll hold their own elections and the elections might not result in a new 
    leader because of vote splitting. So they’ll hold another election. Candidates will hold elections 
    until there’s a winner that becomes the new leader.

    Every Raft server has a term: a monotonically increasing integer that tells other servers how authoritative 
    and current this server is. The servers’ terms act as a logical clock: a way to capture chronological and 
    causal relationships in distributed systems, where real-time clocks are untrustworthy and unimportant. 
    Each time a candidate begins an election, it increments its term. If the candidate wins the election 
    and becomes the leader, the followers update their terms to match and the terms don’t change until the 
    next election. Servers vote once per term for the first candidate that requests votes, as long
    as the candidate’s term is greater than the voters’. 
   ┌─────────────────────────────────────────────────────────────────────────────────────────────┐
     THESE CONDITIONS HELP PREVENT VOTE SPLITS AND ENSURE THE VOTERS ELECT AN UP-TO-DATE LEADER. 
   └─────────────────────────────────────────────────────────────────────────────────────────────┘

   LOG REPLICATOIN: 
     The leader accepts client requests, each of which represents some command to run across the cluster. 
     (In a key-value service for example, you’d have a command to assign a key’s value.) For each request, 
     the leader appends the command to its log and then requests its followers to append the command to their 
     logs. After a majority of followers have replicated the command—when the leader considers the command 
     committed the leader executes the command with a finite-state machine and responds to the client with 
     the result. The leader tracks the highest committed offset and sends this in the requests to its followers.
     When a follower receives a request, it executes all commands up to the highest committed offset with its 
     finite-state machine. All Raft servers run the same finite-state machine that defines how to handle each command

    ┌──────────────────────────────────────────────────────────────────────────────────────────────────────────┐
     The recommended number of servers in a Raft cluster is three and five. A Raft cluster of three           
     servers will tolerate a single server failure while a cluster of five will tolerate two server failures. 
    └──────────────────────────────────────────────────────────────────────────────────────────────────────────┘

  
A Raft instance comprises:
    • A finite-state machine that applies the commands you give Raft;
    • A log store where Raft stores those commands;
    • A stable store where Raft stores the cluster’s confi, servers in the cluster, their addresses, and so on;
    • A snapshot store where Raft stores compact snapshots of its data; and
    • A transport that Raft uses to connect with the server’s peers.
