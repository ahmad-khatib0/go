
****************************************************************************************
# Analyzing or visualizing the project dependencies
$ go mod graph | sed -Ee 's/@[^[:blank:]]+//g' | sort | uniq > unver.txt

# Then, create a graph.dot file containing the following content:
  digraph {
    graph [overlap=false, size=14];
    root="$(go list -m)";
    node [ shape = plaintext, fontname = "Helvetica", fontsize=24];
    "$(go list -m)" [style = filled, fillcolor = "#E94762"];
    
# Previous file content will generate a graph structure using the DOT language. 
# We can use DOT to describe graphs (directed or not). That being said, we will inject 
# the output of unvert.txt into the graph.dot file with the following commands:
$ cat unver.txt | awk '{print "\""$1"\" -> \""$2"\""};' >>graph.dot
$ echo "}" >>graph.dot
$ sed -i '' 's+\("github.com/[^/]*/\)\([^"]*"\)+\1\\n\2+g' graph.dot

# execute the following command to convert the graph.dot file into .svg format:
$ sfdp -Tsvg -o graph.svg graph.dot

****************************************************************************************

# This command will generate the specification in JSON format. 
$ swagger generate spec â€“o ./swagger.json

# load the generated spec in the Swagger UI locally
$ swagger serve ./swagger.json

# load the generated spec in the Swagger UI locally (another theme)
$ swagger serve -F swagger ./swagger.json


# load the recipe.json file directly into the recipes collection 
$ mongoimport --username admin --password password --authenticationDatabase admin \ 
  --db demo --collection recipes --file recipes.json --jsonArray

The Redis container uses the basic caching policy. For production usage, it's recommended to 
configure an eviction policy. You can configure the policy with a redis.conf file:
maxmemory-policymaxmemory 512mb
allkeys-lru

This config allocates 512 MB of memory for Redis and sets the eviction policy to the Least
  Recently Used (LRU) algorithm, which deletes the cache items that were the least recently
  used. As a result, we only keep the items with the highest chances of getting read again

run 2,000 GET requests in total on the /recipes endpoint with 100 concurrent requests:
$ ab -n 2000 -c 100 -g without-cache.data http://localhost:8080/recipes

# the gnuplot utility to plot a chart based on the without-cache.data and with-cache.data files
$ gnuplot apache-benchmark.p



