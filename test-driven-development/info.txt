
The cyclical phases of the TDD working process:
  
1 - We start at the red phase. We begin by considering what we want to test and translating this requirement 
    into a test. Some requirements may be made up of several smaller requirements: at this point, we test only 
    the first small requirement. This test will fail until the new functionality is implemented, giving a name 
    to the red phase. The failing test is key because we want to ensure that the test will fail reliably 
    regardless of what code we write. 

2- Next, we move to the green phase. We swap from test code to implementation, writing just enough code as 
   required to make the failing test pass. The code does not need to be perfect or optimal, but it should be 
   correct enough for the test to pass. It should focus on the requirement tested by the previously written 
   failing test.

3- Finally, we move to the refactor phase. This phase is all about cleaning up both the implementation
   and the test code, removing duplication, and optimizing our solution.

4- We repeat this process until all the requirements are tested and implemented and all tests pass. The developer 
   frequently swaps between testing and implementing code, extending functionality and tests accordingly.


The AAA pattern describes how to structure tests in a uniform manner:
1 - We begin with the Arrange step, which is the setup part of the test. This is when we set up the
    Unit Under Test (UUT) and all of the dependencies that it requires during setup. We also set
    up the inputs and the preconditions used by the test scenario in this section.
2 - Next, the Act step is where we perform the actions specified by the test scenario. Depending on the type 
    of test that we are implementing, this could simply be invoking a function, an external API, or even a 
    database function. This step uses the preconditions and inputs defined in the Arrange step.
3 - Finally, the Assert step is where we confirm that the UUT behaves according to requirements. This
    step compares the output from the UUT with the expected output, as defined by the requirements.
4 - If the Assert step shows that the actual output from the UUT is not as expected, then the test
    is considered failed and the test is finished.
5 - If the Assert step shows that the actual output from the UUT is as expected, then we have two options: 
    one option is that if there are no more test steps, the test is considered passed and the test is finished. 
    The other option is that if there are more test steps, then we go back to the Act step and continue.
6 - The Act and Assert steps can be repeated as many times as necessary for your test scenario.
    However, you should avoid writing lengthy, complicated tests. This is described further in the
    best practices throughout this section.

Tests should cover the following:
  • The functions you implemented
  • The statements that your functions are composed of
  • The different execution paths of your functions
  • The different conditions of your Boolean variables
  • The different parameter values that can be passed to your functions


Functional tests cover the correctness of a system, while non-functional tests cover the usability and performance 
  of a system. Both types of tests are required to ensure that the system satisfies the customers’ needs.

achieving full separation between the source and test code. Using the dedicated test package brings the 
  following advantages:
  
• Prevents brittle tests: Restricting access to only exported functionality does not give test code visibility 
  into package internals, such as state variables, which would otherwise cause inconsistent results.
  
• Separates test and core package dependencies: The test package allows the test to import any dependencies 
  required, without adding those dependencies to the core package. In practice, test code will often have its 
  own dedicated verifiers and functionality, which we would not want to be visible to production code. The 
  test package is a seamless way to guarantee separation.
  
• Allows developers to integrate with their own packages: We previously mentioned that packages allow developers 
  to build their internal code as small APIs. Writing tests from a dedicated test package allows developers to 
  see how easy it is to integrate with their designed external interfaces, ensuring that their code is maintainable.

Here are some of the important types from the testing library that we will be using:

- <testing.T: All tests must use this type to interact with the test runner. It contains a method
  for declaring failing tests, skipping tests, and running tests in parallel.
    
- testing.B: Analogous to the test runner, this type is Go’s benchmark runner. It has the same methods for 
  failing tests, skipping tests, and running benchmarks in parallel. Benchmarks are special kinds of tests that 
  are used for verifying the performance of your code, as opposed to its functionality.

- testing.F: This type is used to set up and run fuzz tests and was added to the Go standard toolchain in Go 
  1.18. It creates a randomized seed for the testing target and works together with the testing.T type to provide 
  test-running functionality. Fuzz tests are special kinds of tests that use random inputs to find edge cases 
  and bugs in our code

a look at the testing.T type: 
  it exposes he following methods for logging, skipping, and failing tests that are important to understand:
  
- t.Log(args): This prints the given arguments to the error log after the test has finished executing.

- t.Logf(format, args): This has the same functionality as the t.Log method, but
  allows the arguments to be formatted before printing.
  
- t.Fail(): This marks the current test as failed but continues execution until the end.

- t.FailNow(): This marks the current test as failed and immediately stops the execution of
  the current test. The next test will be run while continuing the suite.
  
- t.Error(args): This is equivalent to calling t.Log(args) and t.Fail(). This method
  makes it convenient to log an error to the error log and mark the current test as failed.
  
- t.Errorf(format, args): This is equivalent to calling t.Logf(format, args) and t.Fail(). This method 
  makes it convenient to fail a test, then format and print an error line in one call.
  
- t.Fatal(args): This is equivalent to calling t.Log(args) and t.FailNow(). This
  method makes it convenient to fail a test and print an error line in one call.
  
- t.Fatalf(format, args): This is equivalent to calling t.Logf(format, args) and t.FailNow(). This method 
  makes it convenient to fail a test, then format and print an error line in one method call.
  
- t.SkipNow(): This marks the current test as skipped and immediately stops its execution.
  Note that if the test has already been marked as failed, then it remains failed, not skipped.
  
- t.Skip(args): This is equivalent to calling t.Log(args), followed by t.SkipNow(). This method makes 
  it convenient to skip a test and print an error line in one call.
  
- T.Skipf(format, args): This is equivalent to calling t.Logf(format, args), followed by t.SkipNow(). 
  This method makes it convenient to skip a test, then format and print an error line in one call.



- Code coverage
  In order to achieve a high coverage percentage, tests should cover the following:
• The functions you implemented
• The statements that your functions are composed of
• The different execution paths of your functions
• The different conditions of your Boolean variables
• The different parameter values that can be passed to your functions


-- Using the dedicated test package (like format_test in format dir) brings the following advantages:
- Prevents brittle tests: Restricting access to only exported functionality does not give test code 
  visibility into package internals, such as state variables, which would otherwise cause inconsistent results.
- Separates test and core package dependencies: The test package allows the test to import any dependencies 
  required, without adding those dependencies to the core package. In practice, test code will often have 
  its own dedicated verifiers and functionality, which we would not want to be visible to production code. 
  The test package is a seamless way to guarantee separation.
- Allows developers to integrate with their own packages: We previously mentioned that packages allow 
  developers to build their internal code as small APIs. Writing tests from a dedicated test package 
  allows developers to see how easy it is to integrate with their designed external interfaces,
  ensuring that their code is maintainable.


-- Another common approach is to name the tests using a Behavior-Driven Development (BDD) style approach. 
   Performing Integration Testing. In this naming approach, the name of the test follows the structure of 
   TestUnitUnderTest_PreconditionsOrInputs_ExpectedOutput. For example, a test for the function will be 
   named TestAdd_TwoNegativeNumbers_NegativeResults if it tests adding two negative numbers together.


- The test runner supports two running modes:
. When the command has no package specifications, it will build and run all tests in the current directory. 
  This is known as local directory mode. This is how we ran the preceding command using go test –v.
. When the command has package specifications, it will build and run all tests matching the specific 
  package arguments. This is known as package list mode. Developers usually run their tests in this 
  mode for large projects as it can be cumbersome to change between directories and run the tests in 
  each of them using local directory mode.

- We can easily specify what tests to run by providing these properties:
. A specific package name: For example, go test engine_test will run the tests from
  the engine_test package from anywhere in the project directory.
. The expression as the package identifier: For example, go test ./... will run all the
  tests in the project, regardless of where it’s being run from.
. A subdirectory path: For example, go test ./chapter02 will run all the tests in the chapter02 
  subdirectory of the current path, but will not traverse to further nested directories.
. A regular expression, together with the –run flag: For example, go test –run "^engine" will run all packages 
  that begin with the word engine. A subdirectory path can also be provided alongside the test name.
. A test name, together with the –run flag: For example, go test –run TestAdd will
  only the test specified. A subdirectory path can also be provided alongside the test name.

-- The Go test runner can cache successful test results to avoid wasting resources by rerunning tests
   on code that has not changed. Being able to cache successful test results is disabled by default when
   running in local directory mode, but enabled in package list mode.

-- One TestMain function per package:  As names need to be unique inside a package, you will only be 
   able to define one TestMain function per package. You should be mindful that this method will control 
   how ALL the tests inside the given package run, NOT just those in the given file.

Multiple init functions per package
  Unlike other names, multiple init functions are allowed per package. However, you should be mindful that 
  they will all be called before the main runner. When multiple init functions are defined in the same file, 
  they are run in DEFINITION ORDER. On the other hand, when they are defined in multiple files, 
  they are run in the LEXICOGRAPHIC ORDER of their filenames.

$ From The viewpoint of the UUT, the four main types of dependencies are as follows:
- Direct internal dependencies: These contain internal functionality that your UUT imports. These dependencies 
  could be defined in the same package or module as UUT, but are required to deliver its functionality.
- Transitive internal dependencies: These contain internal functionality that the Direct internal dependency 
  parts of your UUT import. These dependencies could also be defined in the same package or module.
- Direct external dependencies: These contain third-party functionality that your UUT imports. These could 
  be libraries or service APIs that you might rely on, but which are not contained in your current module.
- Transitive external dependencies: These contain external functionality that your Direct external 
  dependencies rely on, but which are in a separate module. Due to the way that Go builds the source code 
  and required libraries into runnable executables, these transitive dependencies will also be contained 
  alongside your code during application release.

$ Fundamentally, there are two ways we can go about injecting dependencies:
- Constructor injection: This consists of passing all the required dependencies to a special constructor 
  function, which will then return an instance of the UUT struct. This is an intuitive way to construct 
  instances, but it does require that all dependencies be created before the invocation of the function.  
- Property/method injection: This consists of creating the UUT struct and then setting the fields of the 
  dependencies as you require them. This can either be done by directly setting them as fields on the UUT 
  instance, or by invoking setter methods that set them on the fields. The dependencies are not immutable, so 
  they do not require the UUT instance to be recreated as they are set. This way of creating the UUT and its 
  dependencies does not require that all dependencies be created before initializing and beginning to use the 
  UUT, but it also does not guarantee that all the dependencies will be set by a certain time, nor does it 
  guarantee they won’t be changed later. This could require more application code for nil value checks, as 
  well as other subtle bugs if dependencies change.

Then, each method can be used in two ways:
- Manual invocation: This means that we call and create the UUT struct and its dependencies manually. 
  In this process, you have full control over the creation and invocation of dependencies, but they can 
  become difficult to manage for larger codebases.
- Dependency injection frameworks: This means that you import another dependency into your project that can 
  automate this process using advanced techniques such as reflection or code generation, which then leverage 
  the dependency graph to create the dependencies in the correct sequence. This method is much more 
  sustainable for large codebases.



