 ╒══════════════════════════════════════════════════════════════════════════════════════════╕
   ╒══════════════════════════════════════════════════════════════════════════════════════╕ 
     MallBots application is a modular monolith, which is an application design that sits     
     somewhere between a classic monolith design and a microservices application design.      
     We have most of the benefits of both designs with only a few downsides.                  
   └──────────────────────────────────────────────────────────────────────────────────────┘ 
 ╘══════════════════════════════════════════════════════════════════════════════════════════╛


Three different uses or patterns exist that can be called EDA individually or altogether, as follows:
  • Event notifications
  • Event-carried state transfer
  • Event sourcing


Queues are referred to by a variety of terms, including bus, channel, stream, topic, and others. The
  exact term given to a queue will depend on its use, purpose, and sometimes vendor. Because events
  are frequently—but not always—organized in a first-in, first-out (FIFO) fashion,

Message queues
  The defining characteristic of a message queue is its lack of event retention. All events put into a
  message queue have a limited lifetime. After the events have been consumed or have expired, they are discarded.


Event streams
  When you add event retention to a message queue, you get an event stream. This means consumers
  may now read event streams starting with the earliest event, from a point in the stream representing
  their last read position, or they can begin consuming new events as they are added. Unlike message
  queues, which will eventually return to their default empty state, an event stream will continue to grow
  indefinitely until events are removed by outside forces, such as being configured with a maximum
  stream length or archived based on their age.
  

Event stores
  As the name implies, an event store is an append-only repository for events. Potentially millions of
  individual event streams will exist within an event store. Event stores provide optimistic concurrency
  controls to ensure that each event stream maintains strong consistency. In contrast to the last two
  queue examples, an event store is typically not used for message communication.


-- Component collaboration
   There are two patterns we can use to bring components together to manage workflows, 
   • Choreography: The components each individually know about the work they must do, and
     which step comes next
   • Orchestration: The components know very little about their role and are called on to do their
     part by a centralized orchestrator


 ▲
 █ DDD 
 ▼      

DDD prescribes no specific architecture to use, and it neither instructs you how to 
  organize your code for any given programming language nor enforces any rule that you 
  must use in every corner of your application.

The complexity of the problem domain can be reduced by breaking the domain into subdomains so
  that we’re dealing with more manageable chunks of the problem. Each new domain we identify falls
  into one of three types:

  • Core domains: Critical components of the application that are unique or provide a competitive
      advantage to the business. These get the most focus, the most money, and the best developers.
      A core domain is not always obvious and can evolve or change with the business.

  • Supporting domains: The utility components that provide functionality that supports the
      core business. You might consider using an off-the-shelf solution if what is being provided and
      developed by a team is not specific enough to the business.

  • Generic domains: Components that are unrelated to the core business but necessary for it to
      function. Email, payment processing, reporting, and other common commodity solutions fall
      into this domain type. It wouldn’t make sense to devote teams to develop this functionality
      when so many solutions exist.


The purpose of context mapping is to recognize the relationships the models will have with other
  models and to also show the relationship between teams. The patterns used in context mapping are
  of a descriptive value only. They do not give any hints about what technical implementations should
  exist to connect the models:  (see Figure 2.2 – A context mapping example) : 
  
•• Upstream patterns:
    - Open host service: This context provides an exposed contract that downstream contexts may connect to
    - Event publisher: This context publishes integration events that downstream contexts may subscribe to
 
•• Midway patterns:
    - Shared kernel: Two teams share a subset of the domain model and maybe the database.
    - Published language: A good document shared language to translate models between contexts.
      It is often combined with an open host service.
    - Separate ways: Contexts that have no connections because integration is too expensive.
    - Partnership: A cooperative relationship between two contexts with joint management of the integration.
 
• Downstream patterns:
   - Customer/supplier: A relationship where the downstream context may veto or 
     negotiate changes to the upstream context
   - Conformist: The downstream service is coupled with the upstream context’s model
   - Anticorruption layer: A layer to isolate the downstream context from changes in the upstream context’s model



 ▲
 █ DCA 
 ▼      
 
Domain-centric architectures
  A domain-centric architecture, to reiterate, is an architecture with the domain at the center. Around
  the domain is a layer for application logic, and then around that is a layer for the infrastructure or
  external concerns. The purpose of the architecture is to keep the domain free of any outside influences
  such as database specifics or framework concerns.


-- Hexagonal architecture  (Figure 2.8 – An interpretation of hexagonal architecture with elements of clean architecture)
These pairs of ports and adapters come in two types:
  ••Driver or primary adapters are the web UIs, APIs, and event consumers that drive information
    in our application
  ••Driven or secondary adapters are the database, loggers, and event producers that are driven
    by the application with some information
    
Communication between the adapters and the application happens only through the ports and the
Data Transfer Objects (DTOs) that they have created to represent the requests and responses.



 ▲
 █ CQRS 
 ▼       

Command and Query Responsibility Segregation (CQRS) is a simple pattern to define. Objects
  are split into two new objects, with one being responsible for commands 
  and the other responsible for queries.
The definitions for Command and Query are the same as they are for Command-Query Separation (CQS):
  • Command: Performs a mutation of the application state
  • Query: Returns application state to the caller  


When to consider CQRS Let’s explore the points while considering CQRS:
• Your system experiences a much larger amount of read operations than write operations. Using CQRS 
    allows you to break the operations into different services, allowing them to be scaled independently.
    
• Security is applied differently for writes and reads; it also limits what data is viewable.

• You are using event-driven patterns such as event sourcing. By publishing the events used
   for your event-sourced models, you can generate as many projections as necessary to handle your queries.

• You have complex read patterns that bloat or complicate the model. Moving read models out
   of the domain model allows you to optimize the read models or the storage used for each access pattern

• You want the data to be available when writing is not possible. Whether by choice or not, having
   the reads work when the writes are disabled allows the state of the application to still be returned.


• /root/internal: This package can be imported by /root and any package found in the
    directory tree under it.
• /root/pkg-b/internal: This package may only be imported by /root/pkg-b and
    any package found in the directory tree under it. Both /root and /root/pkg-a will not
    be permitted to use any imports from this package.




• Types of events: 
  In an event-driven application and even in an application that is not event-driven,
  you will encounter several different kinds of events:

Domain events
  A domain event is a concept that comes from domain-driven design. It is used to inform other parts
  of a bounded context about state changes. The events can be handled asynchronously but will most
  often be handled synchronously within the same process that spawned them

Event sourcing events
  An event sourcing event is one that shares a lot in common with a domain event. 
  These events will need to be serialized into a format so that they can be stored in event streams.
  Whereas domain events are only accessible during the duration of the current process, 
  these events are retained for as long as they are needed

Integration events
  An integration event is one that is used to communicate state changes across context 
  boundaries. Like the event sourcing event, it too is serialized into a format that allows it 
  to be shared with other modules and applications. Consumers of these events will need access to 
  information on how to deserialize to use the event at their end. Integration events are strictly 
  asynchronous and use an event broker to decouple the event producer from the event consumers.









What is event sourcing?
  Event sourcing is a pattern of recording each change to an aggregate into an append-only stream. To
  reconstruct an aggregate’s final state, we must read events sequentially and apply them to the aggregate.
  This contrasts with the direct updates made by a create, read, update, and delete (CRUD) system. In
  that system, the changed state of the record is stored in a database that overwrites the prior version
  of the same aggregate.
  Event sourcing implementations should use event stores that provide strong consistency guarantees
  and optimistic concurrency control. That is, when two or more modifications are made concurrently,
  only the first modification can add events to the stream. The rest of the modifications can be retried
  or would simply fail.

Understanding the difference between event streaming and
  event sourcing
  Event streaming is when events are used to communicate state changes with other bounded contexts
  of an application. Event sourcing is when events are used to keep a history of the changes in a single
  context and can be considered an implementation detail and not an architectural choice that has
  application-wide ramifications. These two uses of events are often thought to be the same and some
  speakers, books, and blogs conflate the two or use the terms interchangeably.

  In terms of consistency models, an event streaming system is always going to be eventually consistent.
  An event-sourced system will have the same level of consistency as the database it is used with. With
  an ACID-compliant database, this would be strongly consistent. With non-relational databases, this
  is typically only eventually consistent. Even if event streaming is implemented within the same system
  as a strongly consistent event sourcing system, the former will not compromise the latter’s level of consistency.








What exactly is a message? 
  An event is a message, but a message is not always an event. A message is a container with a payload, 
  which can also be an event and can have some additional information in the form of key-value pairs.
  A message may be used to communicate an event, but it may also be used to communicate an instruction
  or information to another component.

The messages payloads include the following:
  • Integration event: A state change that is communicated outside of its bounded context
  • Command: A request to perform work
  • Query: A request for some information
  • Reply: An informational response to either a command or query

An application uses different kinds of events to accomplish a variety of activities:
 • Domain events: Exist for the shortest time, never leave the application, do not require versioning,
     and are typically handled synchronously. These events are used to inform other application
     components about changes made to or by an aggregate.
 • Event-sourced events: Exist for the longest time, never leave the service boundary, require
     versioning, and are handled synchronously. These events keep a record of every change in state
     that is made to an aggregate.
 • Integration events: Exist for an unknown amount of time, are used by an unknown number
     of consumers, require versioning, and are typically handled asynchronously. These events are used 
     to supply other components of the system with information regarding significant decisions or changes.

A notification event is going to be the smallest event you can send. You might send a notification
  because the volume of the event is very high, or you might send one because the size of the data related
  to the change is too large
Some examples of when to use a notification evetns: 
  • New media has been uploaded or has become available. Serializing the file content into an
    event is not likely to be practical or performant.
  • With events related to time-series data or other tracking events that have a very high volume or rate. 
    Following edits to a large create, read, update, delete (CRUD) resource. Instead of sending
  • the entire updated resource, you might send a list of updated fields only.


Some uses for event-carried state transfer are:
  • Storing shipping addresses for customers in a warehouse service
  • Building a history of product purchases for a seller in an independent search component
  • Information from multiple producers can be combined to create entirely new resources for the
    application to support additional functionality


Ordered message delivery: 
  If the order of the messages causing problems are all related—say, because they belong to the same
  aggregate or the same workflow—then using a partitioned queue will help keep the messages in order
  when they are finally delivered, With a partitioned queue, all messages with the same partition key 
  will be delivered in the order that they were published for that partition. At most, 
  a single consumer will be subscribed to any partition,


-- There are a few more features JetStream adds to NATS Core that we are interested in as outlined here: 
 • Message deduplication: This can deduplicate messages that have been published more than once
 • Message replay: Consumers may receive all messages, or receive messages after a specific point
   in the stream or after a specific timestamp
 • Additional retention policies: We can choose to keep messages if consumers exist with
   subscriptions to them or assign limits on the number of messages or total size of the stream

JetStream provides two components, the Stream and the Consumer: 
• Stream: This is responsible for storing published messages for several subjects. Subjects may
  be named explicitly to be included or be included with the use of token wildcards. Message
  retention—based on duration, size, or interest—is configured independently for each stream.
  Our MallBots stream could be just one stream configured in JetStream alongside many others.
  
• Consumer: This is created as a view on the message store. Each consumer has a cursor that is
  used to iterate over the messages in a stream or a subset of them based on both a subject filter
  and replay policy.









••  The atomicity guarantee ensures that the group of queries is treated as a single unit – that is, a
    single interaction with the database – and that they all either succeed together or fail together

••  The consistency guarantee ensures that the queries transition the state in the database while
    abiding by all rules, constraints, and triggers that exist in the database

••  The isolation guarantee ensures that no other concurrent interactions with the database will
    affect this interaction with the database

••  The durability guarantee ensures that once the transaction has been committed, any state
    changes made by the transaction will survive a system crash

A distributed transaction has the potential to be run over a longer period. Also, some distributed
  transaction choices do not maintain the isolation guarantee so that resources are not blocked and are
  not fully ACID compliant:


Comparing various methods of distributed transactions: 

The 2PC (Two-Phase Commit)
  At the center of a 2PC is a coordinator that sends the Prepare and Commit messages to all the
  participants. During the Prepare phase, each participant may respond positively to signify they have
  started a local transaction and are ready to proceed. If all the participants have responded positively,
  then the coordinator will send a COMMIT message to all of the participants and the distributed
  transaction will be complete. On the other hand, if any participant responds negatively during the
  Prepare phase, then the coordinator will send an ABORT message to inform the other participants
  to roll back their local transaction; again, the distributed transaction will be complete.

  What the 2PC has going against it is big. During the Prepare phase, the participants all create prepared
  transactions that will consume resources until the coordinator gets around to sending the message
  for the Commit phase. If that never arrives for whatever reason, then the participants may end up
  holding open a transaction much longer than they should or may never resolve the transactions.
  Another possibility is that a participant may fail to properly commit the transaction, leaving the
  system in an inconsistent state. Holding onto transactions limits the scalability of this method for
  larger distributed transactions.

The Saga
  A saga is a sequence of steps that define the actions and compensating actions for the system components
  that are involved, also known as the saga participants. In contrast to 2PCs, each participant is not
  expected to use a prepared transaction. Not relying on prepared transactions opens the possibility of
  using NoSQL or another database that does not support prepared transactions. Sagas drop support
  for the isolation guarantee, making them ACD transactions. The saga steps may use a local ACID
  transaction, but any changes that are made will be visible to concurrent operations while the other
  steps are being run.
  Another reason to choose a saga for your distributed transaction is that a saga can be long-lived. Since
  there are no resources tied up in a database blocking other work, we can build a saga that could have
  a lifetime of several seconds, minutes, or even longer....  both types of sagas: 

The Choreographed Saga
  In a choreographed saga, each participant knows the role they play. With no coordinator to tell them
  what to do, each participant listens for the events that signal their turn. The coordination logic is
  spread out across the components and is not centralized.







System under test
  At each level of testing, we use the term system under test (SUT) to describe the component or
  components being tested. For unit tests, the SUT may be a function, whereas for E2E testing, it
  would encompass the application and any external APIs involved. Generally, the SUT expands
  in scope or application coverage the higher up you go in the testing pyramid.

Unit tests
  Unit tests should make an appearance in any testing strategy. They are used to test code for correctness
  and to locate problems with application and business logic implementations. In a testing strategy, they
  should take up the bulk of the testing efforts. These tests should be free of any dependencies, especially
  any I/O, and make use of test doubles such as mocks, stubs, and fakes. The system under test for a
  unit test should be very small; for example, individual functions and methods.

Integration tests
  Instead of focusing on the logic, you will focus on testing the
  interactions between two components. Typically, you must test the interactions between a component
  with one of its dependencies. Testing that your ORM or repository implementations work with a real
  database would be an example of an integration test. Another example would be testing that your web
  interface works with application or business logic components. For an integration test, the SUT will
  be the two components with any additional dependencies replaced with mocks.

Contract tests
  A distributed application or a modular monolith like ours is going to have many connection points
  between the microservices or modules. We can use contract tests built by consumers’ expectations of
  an API or message to verify whether a producer has implemented those expectations. Despite being
  rather high on the testing pyramid, these contract tests are expected to run just as fast as unit tests
  since they do not deal with any real dependencies or test any logic. The SUT for a contract will be
  either the consumer and its expectation, or the producer and its API or message verification.
  Distributed applications will benefit the most from adding contract tests to the testing strategy. These
  tests are not just for testing between microservices – they can also be used to test your UI with its
  backend API.

End-to-end tests
  E2E tests are used to test the expected functionality of the whole application. These tests will include
  the entire application as the SUT. E2E tests are often extensive and slow. If your application includes a
  UI, then that too will become part of the tests because they intend to test the behaviors and correctness
  of the application from the point of view of the end user. The correctness being tested for is how the
  application performs and not like a unit test’s correctness of how the application does it.
  Teams that take on the effort of maintaining fragile and costly tests are rewarded with confidence that
  the application is working as expected during large operations that can span the whole application.
  In the upcoming sections, we will explore each of the testing methods present in our testing strategy

Creating and using test doubles in our tests
  Test doubles are tools we can use to isolate the system or code under test from the rest of the system around it.
  These tools come in different forms, each useful for different testing scenarios:
  
•• Fakes implement the same functionality as the real dependency. An in-memory implementation
   of a repository could stand in and take the place of a PostgreSQL implementation so that the
   test does not rely on any real I/O.
•• Stubs are like fakes, but the stub implementation responds with static or predictable responses.
•• Spies work like an observable proxy of the real implementation. A spy can be used to report
   back the input, return values, and the number of calls that it received. Spies may also help with
   recording the inputs and outputs that were seen for later use.
•• Mocks mimic the real implementation, similar to a fake, but do not provide an alternative
   implementation. Instead, a mock is configured to respond in certain ways to specific inputs.
   Then, like a spy, it can be used to assert whether the proper inputs were received, the right
   number of calls were made, and no unexpected calls or inputs were encountered.


Arrange, act, and assert
  The Arrange-Act-Assert (AAA) pattern is a simple yet powerful way to build your tests. It
  breaks up a test function into three parts for better maintainability and readability. The Arrange
  portion is where the test is set up, the Act portion is where the target is called or interacted
  with, and the Assert portion is where the final test or verification happens. By following this
  pattern, it is easy to spot test functions that are doing more than testing one thing at a time.
  This pattern is also known as Given-When-Then.

Incorporating the dependencies into your tests
  how to incorporate containers into our tests?
  
Option 1 – manually by using Docker Compose files:
  We can create a Docker Compose file for our tests, such as test.docker-compose.yml, that
  will stand up everything we will need to connect to for the integration tests that we’ll write. This
  should make it easy for every developer to have the dependencies available, and so long as everyone
  remembers to start up the environment, they should also have no issues running the tests. Volumes
  can be destroyed during the environment teardown so that previous runs do not affect others.
  The downsides of this option begin with the Compose file itself. If a problem exists when standing
  up the entire environment, then someone will need to make changes to it before they can test. There
  may also be issues running the tests multiple times, so tearing down the environment to stand it back
  up again might be necessary, which could take a considerable amount of time. To tackle this, we can
  take a different approach.

Option 2 – internalizing the Docker integration
  There is a solution we can use that will not only allow us to use different containers or compose
  environments for different tests but also remove the step of having to run a Docker command before
  executing any integration tests.
  Testcontainers-Go (https://golang.testcontainers.org) is a library that makes
  it possible to start up a container or compose an environment that is controlled 
  by code that we can include in our tests.
  The benefits of this option are that we will always have a pristine environment to run our tests in
  and subsequent runs will not need to wait for any containers or volumes to be reset. The other is
  the containers will always be started and removed when the test is run. This means that there is no
  longer any need to maintain documentation on how to prepare a local environment to run tests. This
  is the better option in my opinion, but it will require some initial setup, as well as some resetting or
  cleanup between each test.


Running specific directories, files, or tests: 
To run all the application tests for the Shopping Baskets module, you would use the following command:
  # go test ./baskets/internal/application
  
To run only the RemoveItem test, you would add -run "RemoveItem$" to the command:
  # go test ./baskets/internal/application -run "RemoveItem$"
  
We can target specific table-driven subtests as well. To run only the NoProduct subtest for the
RemoveItem test, we can use "RemoveItem/NoProduct$". For the following command, I
have moved into the internal directory:
  # go test ./application -run "RemoveItem/NoProduct$"
In the previous two command examples, I used a Regex to search for the test to run. You can target
a group of tests with a well-written Regex. The test tool makes it very easy to target specific tests
when we need to be very focused on a test or a collection of tests.


Go build constraints
  We can use the conditional compilation build constraints to create groups of our tests. These constraints
  are normally used to build our programs for different OSs or CPU architectures, but we can also use
  them for our tests because the tests and the application are both compiled when we run the tests.
  Because this is accomplished by adding a special comment to the top of our files, we can only group
  tests together by files; we cannot create any subgroups of the tests within the files
  To group tests into an integration grouping, we can add the following with 
  a second blank line to the top of the tests file:
//go:build integration

The following are a few rules that need to be followed for the compiler to recognize 
  the comment as a build constraint:
• There must not be any spaces between the single-line comment syntax and go:build.
  Multiline comment syntax will not work.
• The constraint must be followed by a blank line.
• The constraint must be the first line in the file.

. To run the tests now, we will need to pass the -tags option into the test command, like so:
  go test ./internal/postgres -tags integration

You can combine multiple tags to create subgroups using the build constraints by 
taking advantage of the Boolean operators that it supports. We can modify the constraint 
so that the database tests are run with all integration tests or can be run by themselves:
  //go:build integration || database
A file with this constraint could be run using any of the following commands:
  go test ./internal/postgres -tags integration
  go test ./internal/postgres -tags database
  go test ./internal/postgres -tags integration,database


Contract testing comes in two forms:
• Consumer-driven contract testing (CDCT), which is when the contract is developed using
  the expectations of the consumers
• Provider-driven contract testing (PDCT), which is when the contract is developed using the
  provided API of the provider








A look at the AWS resources we are deploying
  The AWS resources that we will be deploying are broken up into different files, so let’s run through
  each file and cover the major resources that will be installed and configured by the Terraform code within them:
  
• Application Load Balancer (ALB): The alb.tf file sets up a service account on the Kubernetes
  cluster that will be used by the ALB. The file also contains a Helm resource that will install the
  ALB using a Chart.
 
• Elastic Container Registry (ECR): The ecr.tf file sets up private image registries for each
  of the nine services we will be deploying. It will also build and push each service up into the
  newly created registries.

• EKS: The eks.tf file is responsible for creating our Kubernetes cluster. It makes use of a
  Terraform module, which is a collection of other Terraform scripts, to build the necessary
  resources from one resource definition. Some AWS IAM policies and roles are configured in
  this file for the cluster to support the installation of the ALB.

• RDS: rds.tf will set up a serverless PostgreSQL database and make it available to the
  Kubernetes cluster. The database will also be accessible by us or anyone else who has an IP
  address allowed by the allowed_cidr_block value.

• Security groups: The security_groups.tf file will set up our security group that will
  limit access to our resources from the internet. Whatever allowed_cidr_block we provide
  will be the only set of IP addresses that will be able to reach our database, cluster, and any other
  resources we have set up.

• VPC: The vpc.tf file will create a set of networks, connect them with routing, and also use
  our security group to limit access to them. These networks will be used by the Kubernetes cluster
  to deploy Pods, by the database, and by the application. The VPC will be installed across two
  AZs to improve our deployed resource resiliency by being installed in different data centers




