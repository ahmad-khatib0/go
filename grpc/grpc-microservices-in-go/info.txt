
The main advantage of this framework (grpc) comes from being able to generate server and client stubs 
  ( i.e., objects on the client side that implement the same methods as the service) for multiple 
  languages that can be used both in consumer projects to call remote service methods and in server 
  projects to define business logic behind those service methods.

With the help of gRPC, most of the challenges in microservices, such as handling network failures and applying 
  TLS (Transport Layer Security) to service communications, can be eliminated. By using these built-in features 
  in gRPC, you can improve both the reliability of the product and the productivity of an entire team.

Polyglot development environments are ideal for gRPC integrations since using the
  Python client within the Checkout service to access the Payment service, which is written using Java, 
  is very easy with client stub generation. You can apply the same strategy to your SDK generations for 
  public consumers. Also, whenever you change your service definitions, the test fails on the client side, 
  which is a suitable verification mechanism for your microservices.

If you prefer to expose gRPC without maintaining the SDKs for your consumers, then it is better to share your 
  service definitions with them or provide a clear explanation about how to make gRPC calls to your gRPC services

Microservice architecture opens a gate to the polyglot development environment, which is very helpful for 
  choosing the proper language for different use cases. It also allows the use of various technologies 
  such as Neo4j for graph-related use cases, MySQL if there is a need for relational table structures, or 
  Mongo for document-based data models. Microservice architecture also helps you to construct different small
  teams to assign code ownership to a specific pool of services.

gRPC needs a server address to dial in to call service functions. Kubernetes’s discovery system is a 
  good fit for finding the correct address because the server address is the service name of a microservice 
  defined within a service spec. Suppose you have a proper naming convention for your services. In that case, 
  you also have perfect integration between the consumer and service, with no help from service discovery prod-
  ucts to see the actual address of a specific service.

Let’s say that a consumer uses an SDK to access an API via the API gateway. It
  propagates requests to four to five downstream services to handle all the operations and then returns to 
  the customer. Having a successful response does not mean everything is good; it is not good if there is 
  a latency within this life cycle. After latency detection, request flows can be analyzed by grouping by 
  trace IDs that contain helpful information. Trace IDs in the requests and response headers can be 
  injected quickly with a simple middleware


  gRPC performs well in interservice communications because it uses binary serialization 
   for the data and transfers it through the HTTP/2 protocol.
     
  gRPC allows you to engage in client streaming, server streaming, and bidirectional streaming, 
   which gives you the ability to send multiple requests or receive multiple responses in parallel.
   
  Stable client–server interaction in gRPC microservices is easy because of auto- matic code generation.

  REST is popular primarily because of its broad browser support, but you can still use a gRPC web proxy 
   (e.g., https://github.com/grpc/grpc-web) for REST- to-gRPC conversion.
   
  Due to its high portability, Go is one of the best languages for cloud-native applications, 
   such as microservices in Kubernetes.
   
  Using HTTP/2 over SSL/TLS end-to-end encryption connections in gRPC
   eliminates most of the security concerns for a microservice.


Scale cube
  Plenty of driving factors can force you to change your architecture, and scalability is
  one of them, for performance reasons. A scale cube is a three-dimensional scalability
  model of an application. Those dimensions are X-axis scaling, Y-axis scaling, and Z-axis scaling, 


To have data consistency in a distributed system, you have two options: a two-phase commit (2PC) and saga. 
  2PC coordinates all the processes that form distributed atomic transactions and determines whether they
  should be committed or aborted. A saga is a sequence of local transactions that updates each service and 
  publishes another message to trigger another local transaction on the next service.
  Because transaction steps are spanned across the services, they cannot be handled
  with an annotation or two lines of code. However, there are widely used practices with
  saga, so you don’t need to reinvent the wheel for your use cases. Choreography- and orchestrator-based 
  sagas are the most popular patterns for interservice communication to have consistent data.

Choreography-based saga
  A choreography-based saga is a pattern in which each service executes its local transaction
      and publishes an event to trigger the next service to execute its local transaction.
      Whenever a saga is created, it can be completed in the following patterns:
     Service returns the result to the client once the saga is completed. It receives an
      event to update its domain object’s status as succeeded or failed.
     A saga is created, and the client starts to poll the next service to get either a succeeded or failed 
      response. The unique identifier to start polling should be directly returned when the saga is created.
     A saga is created, and the client uses a WebSocket connection in which the service sends the result back 
      using WebSocket protocol. A saga will be completed once the succeeded or failed result is returned.

Service communications over queue can be handled in two ways:
   Command channels—The publisher sends a message directly to the next service with a replyToChannel 
    parameter so that it can notify the consumer once it completes the operation and commits the transaction. 
    The main drawback of this pattern is that the publisher needs to know the location of the next service.
   Pub/sub mechanism—The publisher publishes a domain event, and interested consumers can consume messages 
    to process and commit a local transaction. The main disadvantage of this notation is that it is a possible 
    single point of failure since all the subscribers use one broker technology and all the events are
    sent to consumers.

An orchestration-based saga consists of an orchestrator and participants, and the orchestrator tells 
  participants what to do. The orchestrator can communicate with participants using a command channel or
  request/response style. It connects participants individually to tell them to execute their local 
  transactions and decides the next step based on this response.

Service discovery
  Service discovery is the operation in which service locations are managed and exposed outside to 
  let each service find the next one for step execution. There are two types of service discovery:
   Client-side service discovery—In this notation, a service discovery tool allows applications to report 
    their locations during startup. Client applications have direct connections to the service registry, 
    and they query the location of a specific service by providing some criteria, like the service name,
    or a unique identifier.
   Server-side service discovery—A load balancer integrates with the service registry to resolve downstream 
    services. Client applications connect to services via the load balancer instead of using the service 
    registry to resolve the exact location.

the order creation flow to use gRPC and protocol buffers for interservice
  communication. The ideal steps are as follows:
  
1 Define the proto files that contain message and service definitions. These can be
  inside the current project or in a separate repository independently maintained.
2 Generate the client and server stubs from the .proto file.
3 Implement the server-side business logic by using one of the supported languages (https://www.grpc.io/docs/languages/).
4 Implement the client-side business logic to connect services through the stub.
5 Run the service and client.


Protobuf FIELD NAMES
  The Protobuf compiler requires naming conventions for field naming since it generates source code for 
  multiple languages by relying on those rules. The field name should be lowercase; if it contains more than 
  one word, it should be separated by an underscore (e.g., user_id).

FIELD NUMBERS
  Each field has a unique identifier in the message to identify the field in the binary message format. Since 
  field numbers are unique identifiers for the fields, those numbers shouldn’t be changed to provide a backward 
  compatibility guarantee. If you want to remove a field, it is best to reserve it with the reserved keyword 
  before removing it to prevent future definitions with the same field number or name. You can also reserve those 
  fields by using their field numbers one by one or by a range with the help of the to keyword. For example, 
  removing the customer_id field with field number 3 and adding a new field with the same field name or number with 
  a different type will cause problems. If the client has the old message definition and the server has the new one, a data compatibility problem will result because they contain the same field but the data types are different

message CreateOrderRequest {
    reserved 1, 2, 3 to 7;          // Reservation by single or ranged numbers such as 3 to 7
    reserved "customer_id";         // customer_id is reserved since user_id is introduced.
    int64 user_id = 7;
    repeated Item items = 8;
    float amount = 9;
}


Required fields in a message can be thought of as frequently used fields since you can not skip them as you 
  can for optional fields. It is a best practice to reserve some numbers between 1 and 15 for the fields that 
  can be frequently used since the numbers take 1 byte to encode in that range. For example, if you introduce a 
  field with the name correlation_id, and it is used in almost all types of requests, you can assign one of the
  pre-reserved numbers for this new field. In the same way, it takes 2 bytes to encode numbers from 16 to 2,047. 
  Giving frequently used fields numbers between 1 and 15 will increase performance quality.


GRPC. Marshalling results in some bytes containing encoding information of the metadata and the data itself: 
1 The metadata section is expressed with 1 byte and has the first three bits for denoting the wire type: 000, 
   which is type 0 (Varint) since our data type is int (see the whole list below in the table). 
2 The first bit of the data section is called the most significant bit (MSB), and its value is 0 when there is 
  no additional byte. Its value becomes 1 if more bytes come to encode the remaining data.
3 The remaining bits of the metadata section contain the field value.
4 The data section contains the MSB (i.e., a continuation bit) to state whether there are more bytes.
5 The remaining seven bits are used for the data itself.


There are six wire types: VARINT, I64, LEN, SGROUP, EGROUP, and I32: 
  ID	Name	       Used For
  0	  VARINT	     int32, int64, uint32, uint64, sint32, sint64, bool, enum
  1	  I64	         fixed64, sfixed64, double
  2	  LEN	         string, bytes, embedded messages, packed repeated fields
  3	  SGROUP	     group start (deprecated)
  4	  EGROUP	     group end (deprecated)
  5	  I32	         fixed32, sfixed32, float

A field’s value can be anything based on your needs, and thus cannot affect perfor- mance. However, we can 
  affect performance by following some rules for field num- bers. For example, you can use numbers less than 
  or equal to 15 for field numbers since that is the maximum number a metadata block can store. More metadata blocks 
  are needed to express a specified field number. In the same way, if you want to store a data value greater 
  than 127 (the maximum capacity of a data block), you need more bytes to fit that value in those data blocks.


how to encode an object with a value greater than 127 ? Let’s say that the CreatePaymentRequest message has only 
  one field, user_id, with type int, and field number 2. We compiled this message and used it in our production code:
  
  // order.proto
  message CreatePaymentRequest { int64 user_id = 2; }

  // main.go
  request := CreatePaymentRequest { UserId: 21567 }
  // send a request via gRPC

The Protobuf compiler will marshal the request object into a []byte, and the metadata section will be encoded, 
  just like the previous example and as visualized in figure 3.2. The data section will be handled this way:
  
1 Convert the decimal value 21567 to a binary value: 101010000111111.

2 Split the binary value into seven-bit blocks: 0000001-0101000-0111111.

3 The seven-bit block is for data, and the last bit will be used to store MSB.

4 Reverse the order of data parts (https://betterexplained.com/articles/understanding-big-and-little-endian-byte-order/), 
  which will result in 0111111-0101000-0000001.
  
5 Since there are three data parts here, the first will have the MSB as 1, the second as 1, 
  and the 3rd as 0 since no more bytes come after that.

protoc mainly accepts the following parameters to generate language-specific source code:
  -I—To specify the import path where imported packages in .proto files are searched

  --go_ou—To specify where to put generated Go code for messages

  --go_opt—To configure options for Go source code generation, such as paths=source_relative, to keep 
     the same folder structure after source code generation
     
  --go-grpc_out—To define the destination folder of gRPC-specific 
     Go source code, such as calling a service function
     
  --go-grpc_opt—To configure options for gRPC-related operations, such as paths=source_relative, 
     to have the same folder structure after source code generation


Proto project structure
  Assume you created a repository, github.com/username/microservices-proto, with dedicated folders for each 
  service to store .proto files. This project will have the following characteristics:
  proto files are grouped by service folders such as order/order.proto in the root folder.

  There is a folder inside the root project for each language to store language-specific implementations.

  Generated source code for each service will be formatted as a typical Go module project since it 
   will be added as a dependency on the consumer side. – As an example, the module name of the Order service 
   will be github.com/username/microservices-proto/golang/order.
   
  Generated source code will be tagged: golang/<service_name>/<version> (e.g. golang/order/v1.2.3). 
   This is the convention for the Go module to resolve the
   dependency that lives in subfolders in the remote repository:


Removing a field from oneof is a backward-incompatible change, and adding a new field to oneof is a forward-incompatible 
  change. If there is an incompatible change in your message field, you need to introduce an update to your semantic 
  version (https://semver.org/) so that consumers will know there is a breaking change. Consumers will need to check the 
  release notes page of the new release to make the necessary changes on the client side and avoid compatibility problems.



Hexagonal architecture: 
Alistair Cockburn in 2005, is an architectural pattern that aims to build loosely coupled application components that 
  can be connected via ports and adapters. In this pattern, the consumer opens the application at a port via an adapter,
  and the output is sent through a port to an adapter. Therefore, hexagonal architecture is also known as a ports and adapters 
  system. Using ports and adapters creates an abstraction layer that isolates the application’s core from external dependencies

Application
  An application is a technology-agnostic component that contains the business logic that orchestrates functionalities or 
  use cases. A hexagon represents the application that receives write and read queries from the ports and sends them to external 
  actors, such as database and third-party services, via ports. A hexagon visually represents multiple port/adapter combinations 
  for an application and shows the difference between the left side (or driving side) and right side (or driven side).

Actors
  Actors are designed to interact with humans, other applications, and any other software or hardware device. There 
  are two types of actors: driver (or primary) and driven (or secondary). Driver actors are responsible for triggering 
  communication with the application to invoke a service on it. Command-line interfaces (CLIs), controllers, are good 
  examples of driver actors since they take user input and send it to the application via a port. Driven actors expect to 
  see communication triggered by the application itself. For example, an application triggers a communication 
  to save data into MySQL.

Ports
  Ports are generally interfaces that contain information about interactions between an actor and an application. 
  Driver ports have a set of actions, and actors should imple- ment them. Driver ports contain a set of actions 
  that the application provides and exposes to the public.

Adapters
  Adapters deal primarily with transforming a request from an actor to an application, and vice versa. Data transformation 
  helps the application understand the requests that come from actors. For example, a specific driver adapter can transform 
  a technology-specific request into a call to an application service. In the same way, a driven adapter can convert a 
  technology-agnostic request from the application into a technology-specific request on the driven port.


Project folders
  While there are no written rules for a hexagonal architecture folder, the following
  folders are common in typical Go projects:
    Application folder—This folder contains microservice business logic, which is a combination of the domain model 
     that refers to a business entity and an API that exposes core functionalities to other modules.
     
    Port folder—This folder contains contract information for integration between the core application and third parties. 
     This can be a contract about accessing core application features or about specifying available features for a database
     system, if one is used for the persistence layer.
     
    Adapter folder—This folder contains concrete implementation for using contracts that are defined in ports. 
     For example, gRPC can be an adapter with a concrete implementation that handles requests and uses an API port to access
     core functionalities, such as if you have an application with some functionalities and will expose it to customers. 
     The functionalities can be CreateProduct, GetProduct, and so on, and you can expose them to the customer via REST,
     gRPC, and other adaptors, which will use the contracts of those functionalities, as defined in the port layer. 
     We will revisit this topic and look at more advanced examples in later sections of this chapter.

Dependency injection and running the application
The 12-factor app is a methodology for building applications that encourages you to: 
    Use a declarative setup for infrastructure and for application environment automation to 
     quickly deploy to any environment, such as dev, staging, or prod
     
    Have a clean contract between underlying operating systems so that the same
     application can be executed on any environment with different parameters
     
    Use continuous deployment to minimize divergence between environments
  
    Scale easily without any significant change in the system.


gRPC uses predefined status codes within the RPC protocol that are understood among different languages. For example, 
  for successful operations, gRPC returns an OK status code. All the remaining codes are about unsuccessful use cases:
    
    CANCELLED—In this use case, the client calls the server, and for a specific reason, it cancels the request. For 
     example, you call multiple services, and for whichever returns first, you use that data and cancel the other requests.
     
    INVALID_ARGUMENT—This status code is caused by the caller in that it provides invalid input, and the server complains 
     about that. For example, the server will return this status code if you provide an empty order ID during payment creation.
     
    DEADLINE_EXCEEDED—This status code shows that the deadline expired before the actual operation could complete. For 
     example, say you configure your client to have a deadline of 5 seconds. If you call an endpoint with this client and it 
     takes 6 seconds to complete, you will see it will get DEADLINE_EXCEEDED after 5 seconds before the actual operation finishes.
     
    NOT_FOUND— This status code states that a resource is not found. For example, you want to get order details by ID, but 
     you get NOT_FOUND since there is no order with that ID.
  
    ALREADY_EXISTS—This status code is for preventing resource duplication. For example, if you try creating a user 
     with an existing email address, the server will return this status code.
  
    PERMISSION_DENIED— If the operation is not allowed for the current caller, the server will return this status code. 
     You might be already logged into the system, but the resource you are trying to access may need higher permissions.
     
    RESOURCE_EXHAUSTED—This code is used once the caller reaches its limit for usage. For example, you may have a quota 
     for a Software as a Service (SaaS) product; then, once you reach the limit for the product 
     in that environment, the server will return this status code.
     
    INTERNAL—This status code is used for internal server errors.


Assume that you have a 50 MS average latency limit for your interservice communication, which sometimes exceeds this limit. 
  You can apply resiliency patterns to this use case: once the average request latency exceeds the limit, you halt sending
  the request for a while and then resume after a specific time. This prevents memory leaks by dropping long-lived 
  connections to other services, which can cause a possible cascading failure

In gRPC, there are two types of interceptor usage, WithUnaryInterceptor and With StreamingInterceptor for unary and streaming 
  connections, respectively. As in listing 6.3, you can use UnaryClientInterceptor with or without values; if you don’t
  pass a value, it will use default values for retry, or you can override them by using additional configurations such as 
  WithCodes, WithMax, or WithBackoff. Keep in mind that the gRPC retry configuration is handled via the grpc_retry package.
  WithCodes is used for deciding when to retry, and its default value is the total of the Unavailable and ResourceExhausted 
  lists. Unavailable code is the default since retrying until the service becomes available is beneficial for the client to 
  recover the gRPC call once the dependent service becomes available. In the same way, ResourceExhausted is a default because 
  the client might have performed multiple calls that caused the server to apply throttling. For this kind of case, the 
  server will remove throttling, and the client will succeed on the next calls performed by the retry. With these defaults, 
  the retry pattern is applied only if you get Unavailable or ResourceExhausted from the service call. The WithMax parameter 
  helps to set a maximum limit for retries during interservice communication. If the dependent service becomes available earlier,
  the client will not retry until the maximum count; it will stop once the client starts to get a response code other than 
  Unavailable or ResourceExhausted. WithBackoff requires a function to specify back-off functionality between retries. In
  our example, BackoffLinear is used, which means there is a one-second wait time between retries. There are other options, 
  such as BackoffExponential, in which the timeout interval on the next retry is multiplied by two, compared to the current 
  timeout interval. Other options are BackoffLinearWithJitter and BackoffExponentialWithJitter, in which the next timeout 
  interval is decided randomly. This randomness reduces the collision of gRPC calls clients make


Circuit breaker pattern
  In the circuit breaker pattern, connections between services are also called circuits, and if the error rate of an 
  interservice communication reaches a threshold value, it opens that circuit, which means the connection between two 
  services is closed intentionally. After this point, any request to the dependent service will immediately fail due to the
  open circuit. Of course, this is not a permanent situation; it will be reset after a certain time based on your 
  configuration, and if there is no failure in new requests, the circuit will be closed again. Once the reset timeout 
  is triggered, the state will go into a half-open state, and become closed if there is no other failure. 
  If it continues to get failures, it will go back into an open state

The relevant terms in a circuit breaker implementation:
  MaxRequests limits the number of requests allowed to pass through a half-open circuit. The difference between an 
   open and half-open circuit is that a half-open one allows certain requests to pass through the circuit based on 
   your configuration, whereas you can’t pass through an open request.
   
  Interval mainly defines when the count is cleared while the state is closed. For some use cases, you may not want to 
   clear the interval, even if it has been a long time since you last saw a failure. However, this interval is cleared 
   for most use cases to allow users to make failures within a reasonable time interval.
     
  Timeout decides when to change a circuit breaker state from an open to a half-open state. The advantage of this 
   change is that in an open state, the requests will fail fast, whereas in a half-open state, the circuit breaker 
   allows passing through a half-open circuit by limiting the number of requests.
   
  ReadyToTrip checks the state of the failure threshold after the last failure and 
   decides on whether to open the circuit completely.
   
  OnStateChange is mainly used for tracking state changes while handling business models within a function wrapper.


gRPC TLS credentials
  Adding certificate information to a server implementation is twofold: implement logic
  to load credentials and create a TransportCredentials (http://mng.bz/gBAe) instance;
  then use this function within the interceptor to handle credentials verification out of
  the box for each request. This means the following steps are applied:
1 The client sends a gRPC call to the server.
2 The server presents its shared certificate with its public key.
3 The client validates this certificate on a CA. For now, the CA cert contains client and server shared certificates.
4 After client validation, the client presents its shared certificate with its public key to the server.
5 The server validates the shared certificate on the CA.
6 After successful verification, the client receives a response from the gRPC call


GRPC resiliency Summary
 In a typical microservices architecture, it is normal for one service to depend on one or more other services. 
  If one of the dependent services is down, it will affect the availability of the entire microservice application. 
  We use resiliency patterns such as retry, timeout, and circuit breaker to prevent these situations.
  
  Once the dependent service is down, we can use the retry strategy on the con-
   sumer side to make gRPC communication eventually succeed.
  Retry logic can be triggered for certain status codes, such as Unavailable or ResourceExhausted, instead of blindly 
   retrying on each failure. For example, it is not wise to retry a request if you received a validation exception 
   because it shows that you sent an invalid payload; you should fix it to make it succeed.
   
  Using retry logic blocks the actual gRPC call since you have to wait for dependent service. It is hard to detect 
   recovery time for a dependent service, which can create long wait times for retries. To prevent this situation, 
   we use context timeout and deadline to put a time limit on blocking the actual execution.
   
  In the retry mechanism, you can redo an operation for specific time intervals, but this can also put an extra load 
   on the dependent service since it has to retry all the time, even if the dependent service is not ready. To solve 
   this problem, we use a circuit breaker to open a circuit once we reach the failure limit, retry
   the request after some time, and finally close the circuit once the dependent service is back online.

  Error handling is important in interservice communication because the next step is decided from error 
   codes or messages in the gRPC response. We use a status package to return customized errors from a service, 
   and we can convert them on the client side once needed.

  Resiliency is important not only in communication patterns, but also in zero-trust environments, in which we 
   use TLS-enabled communications, as the server and client verify their certificates during gRPC communications. 
   This is also called mutual TLS.

Testing pyramid
  The testing pyramid organizes software tests into three categories based on their context 
  and provides insight into the percentage of tests for each category:
  Unit tests—Type of software test in which specific units of software are tested
  Integration tests—Type of software test in which the integration of multiple mod- ules is tested
  End-to-end tests—Type of software test in which the entire behavior of the application is tested

As you can see, in a typical application, the percentage of unit tests is greater than that for integration tests, 
  which is greater than the percentage for end-to-end tests. There are possible reasons/outputs for that percentage; 
  let’s analyze them individually. Unit tests are designed to test one component at a time, with maximum isolation.
  While testing a component (SUT), you should mock the other dependencies. Isolation levels get lower once you move 
  up on the pyramid because you start to involve more components in the test suite that might break the isolation.
  In a unit test, you probably have a test runner, enough to test the core features and mock the dependencies. Once you 
  move to the integration test, you need third-party tools to maintain dependencies, such as having test containers for 
  a DB connection. Once you start to use third parties, test execution will slow down to wait for all the dependent components.
  Going from a unit test to an end-to-end test has cost increases because more components mean more resource consumption, 
  and thus more money. The unit test has the greatest percentage of the pyramid because it is fast and cheap. In the same way,
  the end-to-end test has the lowest percentage of the pyramid because it is expensive and slow to run. This does not mean 
  you need to write a unit test but not an end-to-end test. It does mean you should arrange the percentage of test types 
  as stated in the test pyramid. Now that we can see the relations between testing strategies, let’s look at
  how those strategies are used to verify the microservice application behavior.

System under test
  The SUT test contains inputs, execution conditions, and expected results to verify its behavior within a codebase. 
  A SUT means a software element is being tested. Based on your testing strategy, a SUT can be a class or an entire 
  application. This test is important if you are testing a specific layer in hexagonal architecture because you need to 
  know what to test in that layer. It becomes a test suite if you group related tests to verify the behavior of a SUT.


The phases of an automated test:
1 Setup—In this phase, we prepare the dependencies of a SUT and initialize a SUT with them. 
  This can also involve initializing third-party dependencies, such as a MySQL database.
2 Invoke the SUT—If we are testing a class, in this phase, we might call a function from that class.
3 Verify—Verify the actual result with the expected result by using assertions.
4 Teardown—Clean up resources that are no longer needed. For example, we
  could destroy the MySQL database once we are done with it.

Working with mocks
  Mocking in a test aims to isolate dependent systems’ internals and focus only on a SUT to not only have minimal 
  setup for your test, but to also control the behavior of the dependent system based on your needs. Let’s say 
  that the Order service depends on the Payment service and Order database, and you want to test one of the scenarios
  for the PlaceOrder functionality in the Order service. If you want to control the behavior of the Order database and 
  Payment service operations, you can mock the Order database and Payment service and then simulate the method calls

We can use the following steps to create a mock for any interface:
  1 Create a mock struct for the payment interface.
  2 Embed mock.Mock as a field to this struct.
  3 Create a receiver function for the Charge method.
  4 Create a mock struct for the DB interface.
  5 Create a receiver function for the Save and Get functions that have the same signature as stated in a real interface.

Automatic mock generation
  Hexagonal architecture encourages defining your ports as an interface, then implementing the adapters afterward. 
  It is easy to mock interfaces because mocking libraries need exposed functions of an API. If there are lots of 
  interfaces in your project to mock, you can use mockery (https://github.com/vektra/mockery)
  execute the following command/:         mockery --all --keeptree  
  You will see autogenerated files, such as *_mock.go, for each interface that contains the mocks. Instead of trying 
  to mock method arguments and return values, mockery does that for us, and we can use those mocks in our unit tests. 
  When they are needed, we can re-create mocks whenever we update an interface or introduce a new one. The
  flow is the same after generating mocks so that you can control their behavior and test a SUT.


You may want to act on the following cases in a test suite:
  Before each test—You may want to create a mock before each test instead of manually defining that mock on each test 
   function. We can run an action before each test by adding a method named TestSetupTest to our test suite.
  After each test—Let’s say you interact with files and folders on each test and want to delete them to have a new 
   state on the next test. We can activate such an action by implementing TestTearDownSuite.
  Before all the tests—We prepare resources for all tests to use because they use a shared resource. For example, we can 
   initialize a database via Testcontainers before running the tests. This action can be activated by implementing TestSetupSuite.
  After all the tests—Let’s say you initialized your DB instance and want to destroy it right after you finish all the 
   tests. We can trigger this action by adding a method named TestTearDownSuite.


In a typical Testcontainer setup, you can see the following events:
  A background context
  A container request with an image, exposed ports, and a liveness probe statement that the Testcontainers 
   library can use to check whether the running container is ready to accept connections
  Once the container is up and running, a connection URL to use in our tests


A Kubernetes cluster is a distributed system made from a machine set that contains primarily containerized workloads. 
  Those nodes are divided into control plane nodes and worker nodes. The control plane is the brain of the Kubernetes 
  cluster, and it has mission-critical components to manage resources in the Kubernetes environment. Those resources 
  are specific to Kubernetes, but they manage a containerized application under the hood. Worker nodes contain system 
  components like kubelet and work-loads like pods, the minimum deployable unit in Kubernetes

RollingUpdate
  RollingUpdate is a deployment strategy in which a new ReplicaSet is created for the new version, and the old 
  ReplicaSet is gradually removed until the new one has the correct number of replicas. This is the default behavior 
  of a deployment in Kubernetes (http://mng.bz/0KQW). Assume that we have huseyinbabal/order:1.0.0 in our deployment, 
  and you want to deploy a new version of 1.1.0. You can trigger a rolling update with the following command:
     $ kubectl set image deployment/order order=order:1.1.0

Blue-Green Deployment
  In this type of deployment strategy, there are two versions of the existing system, and whenever a deployment occurs 
  on one of the systems, the old one becomes deprecated by routing traffic to the new system. We currently have Payment 
  Service v1 in production and plan to deploy v2 soon. However, this time it will not be like an almost-instant update 
  like in RollingUpdate; v2 will be prepared in advance, and traffic will be switched to v2 after everything is taken 
  care of. In this case, we start with Green (v1), and when v2 is deployed, it is Blue at the beginning, but after a 
  while will be Green.

in Blue-Green deployment what happens under the hood is this: 
  Remember, once we create an Ingress controller, it provisions a load balancer,
   and we add Ingress resources behind this controller using the Ingress name.
   
  For the blue environment, an additional Ingress controller can be deployed, and Ingress v2 can be involved 
   behind the load balancer created by the new Ingress controller.
   
  The current state is our domain has a record pointing to the initial load balancer (green), and once we 
   feel confident, we simply update the record value as the second load balancer.
   
  Once we introduce a new version, it is deployed behind the first load balancer
   and becomes green after an internal check.
   
  Most of the time, blue environments are accessible from the internal network so that they can be tested by internal 
   employees or a particular group of testers. This deployment strategy is smoother since we simply change the load 
   balancer switch. However, since we duplicate the existing system entirely as v2, it will introduce an additional cost.

Canary deployment
  Remember that a Service resource is designed to expose a pod as an endpoint internally or publicly to users. 
  The Service–pod relationship is handled by selectors in the Service resource so that Service uses a selector 
  field to find available pods filtered by the logic inside the selector. In canary deployment, there is always 
  one service, but this time an additional deployment is created with new data (e.g., a new Docker image) but with 
  the same selector. Then, requests are distributed between v1 and v2 pods. In canary deployment, this percentage is 
  gradually changed until all the replicas become available in the new deployment and the old one is downscaled to zero 
  In figure 8.11, you can see the transitions for canary deployment, which are simply
  a summary of gradually increasing the possibility of seeing the newly deployed feature
  while deprecating the old deployment. We need a second deployment here with the
  same selector so that once the request reaches the load balancer, it will be routed to a
  pod by selectors. Canary deployment is widely used, especially for collecting feedback
  from end users by enabling an experimental feature to a subset of customers instead
  of overwriting the current production.




Traces
  A trace, a collection of operations to handle a unique transaction, refers to the journey of a request across 
  services in a distributed system. It encodes the metadata of a specific request to propagate it until it reaches 
  its final service. A trace can contain one or more spans that map to a single operation

Metrics
  Metrics contain numerical values to help us define a service’s behavior over time. Prometheus has metrics that are 
  defined by name, value, label, and timestamp. Using these fields, we can see metrics over time. We can also group 
  them using their labels. Of course, we can apply other aggregation techniques (https://prometheus.io/docs/prometheus/latest/querying/functions/) 
  using metric fields, SLA, SLO, and SLI, and obtain information about the system. For more in-depth information, see Google’s
  Site Reliability Engineering book (https://sre.google/sre-book/table-of-contents/).
  
SLA
  An SLA (service-level agreement) is an agreement between customers and service providers and 
  contains measurable metrics such as latency, throughput, and uptime. It is not easy to measure 
  and report SLAs, so a stable observability system is important.
  
SLI
  An SLI (service-level indicator) is a specific metric that helps showcase service quality to customers by referencing 
  request latency, error rate, and throughput for example. The statement “The throughput of our service is 1000/ms” 
  indicates that this service can handle 1,000 operations per millisecond.
  
SLO
  A SLO (service-level objective) is the goal for a product team to satisfy SLA. Especially in SaaS projects, you can 
  see the SLA in terms and conditions pages. A typical example of a SLO is “99.999% uptime for the Object Storage 
  service”: the Object Storage service should be up 99.999%, and it might be down 0.001% of the time. These numbers are
  calculated as average values 

In SLO the average Response latency result is found by adding latency amounts (ms) and dividing them by the sampling 
  count of 10. This is also called the average value of num- bers. If I was using SLA documentation, I would say, 
  “We provide a 4 ms response latency guarantee for our services.” Can you spot the problem here? What if you have
  a 1,000 times 1 ms response latency and ten times 4 seconds latency?
   ▲                                         ▲
   █ 1000 x 1 ms + 10 x 4000 ms = 5000 ms    █
   █ 5000,ms / 1010 = 5ms                    █
   ▼                                         ▼
  Even though you had a very good response latency of 1 ms, the remaining ten response latencies corrupted your report. 
  Averaging numbers may not satisfy custom- ers; they might want to see the distribution of latencies with their 
  percentages. There this a term to explain this situation: percentile. For example, if you say, “The 95 percentile 
  response latency is 3 ms,” 95% of the response latencies are 3 ms or less.
  
  Think about the response latency numbers in:    
          10 22 2 3 9 1 87 11 7 4
  The first thing we can do is sort all the num bers in descending order, which results in the
          87 22 11 10 9 7 4 3 2 1
  To find 80 percentile response latency, see the index at 80% from the right to left
  direction, which in this case is 11 ms.


LOGS
 In a Kubernetes environment, we have two types of logging architecture:
       Node-level logging
       Cluster-level logging

NODE-LEVEL LOGGING
  In node-level logging architecture, applications generate events, which are logged to a file or standard output using 
  some logging library in the application. If the application keeps logging events, that file can grow dramatically, 
  which will make it hard to find certain logs. To avoid this situation, we can use log rotation: once a log file 
  reaches a specific size, it can be rotated to a file with a name that contains time metadata for that rotation. Once 
  you rotate logs, they are saved in separate files, but we still need to ship them to a central location or find a 
  way to analyze them in multiple files

CLUSTER LEVEL LOGGING
  There are agents, typically a daemonset, on each Kubernetes cluster node that are responsible for collecting logs
  from container-standard output logs. In cluster-level logging architecture, the logs are saved to a file on the host, 
  but this time a central agent, (e.g., Elasticsearch, https://www.elastic.co/; graylog, https://www.graylog.org/)
  collects those logs and sends them to the logging backend. 

ELK stack, Elasticsearch-Logstash-Kibana: Elasticsearch is the logging backend, Logstash is some kind of log collector, 
  and Kibana is the UI for logs, a proper monitoring dashboard for gRPC microservices that gives insight into services 
  such as error rates and check details. We can even create alarms to send a notification to the development team if 
  there is a matching log pattern in the logging backend store. 


OpenTelemetry Collector
  OpenTelemetry Collector uses otel/opentelemetry-collector-contrib (https://hub.docker.com/r/otel/opentelemetry-collector-contrib), 
  which exposes two ports: 14278 and 8889. 14278 is for the collector endpoint; it accepts metric requests from gRPC
  microservices. 8889 is used for the Prometheus exporter, and Jaeger uses this port to get time series data for 
  performance monitoring. Once the collector obtains the values, it also sends calculated data to Prometheus.

We can add the following environment variables to the Jaeger All in One deployment to enable the 
  Prometheus metrics storage type:
        METRICS_STORAGE_TYPE=prometheus
        PROMETHEUS_SERVER_URL=http://jaeger-prometheus.jaeger.svc.cluster.local:9090
  jaeger-prometheus is the Prometheus service name, jaeger is the namespace, and
  svc.cluster.local is the suffix used for Kubernetes service discovery.

OpenTelemetry SDKs sends data to the collector endpoint
(http://jaeger-otel.jaeger.svc.cluster.local:14278/api/traces). Traces are sent to the OpenTelemetry Collector.




